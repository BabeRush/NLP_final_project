{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMDhNHUPeOJSZ9nCxpQRPYq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BabeRush/NLP_final_project/blob/main/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Wb80KGrq4u2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notebook preparation"
      ],
      "metadata": {
        "id": "S5o-iTkq4zLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configurations"
      ],
      "metadata": {
        "id": "DWmudSpX6gv9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking high RAM from google\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atwulLF86qMV",
        "outputId": "82a5d747-3371-4e92-90b6-5fc0293ad69d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 89.6 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning and checking for GPU usage\n",
        "import torch\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IOrLnY3k8GbP",
        "outputId": "089ff700-0d3d-482b-a06c-97acbb37eee7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "jIx1EQ6r8bxf"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports & Installations"
      ],
      "metadata": {
        "id": "quzhpRQ769E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZxRBMFD4x9w",
        "outputId": "230804aa-637a-44cc-c7d7-8e7cec64080d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.0-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTS\n",
        "# General\n",
        "import warnings\n",
        "import random\n",
        "\n",
        "# Math & Data organization\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Deep Learning\n",
        "import transformers\n",
        "from transformers import AutoModel, BertTokenizerFast , BertForSequenceClassification\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Visualization\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from tabulate import tabulate\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Working with files\n",
        "from google.colab import files\n",
        "import pickle"
      ],
      "metadata": {
        "id": "Wb8Yc_8S5Be7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the datasets"
      ],
      "metadata": {
        "id": "FHXGntwn8tNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the datasets\n",
        "# Description: ************TODO-LINK******************\n",
        "!git clone https://github.com/alonbarnathan3/RUNI-NLP-FINAL-PROJECT.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlwd4QMX8zJ7",
        "outputId": "a1b4340f-9cfc-4ae2-abc1-1957788b1e59"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'RUNI-NLP-FINAL-PROJECT'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 21 (delta 0), reused 1 (delta 0), pack-reused 19\u001b[K\n",
            "Receiving objects: 100% (21/21), 61.70 MiB | 17.13 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n",
            "Updating files: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_real = pd.read_csv('/content/RUNI-NLP-FINAL-PROJECT/True.csv',encoding='utf-8')\n",
        "df_fake = pd.read_csv ('/content/RUNI-NLP-FINAL-PROJECT/Fake.csv',encoding='utf-8')\n",
        "\n",
        "# reading these tsv files: seperated by tabs, no header\n",
        "liar_train = pd.read_csv('/content/RUNI-NLP-FINAL-PROJECT/train.tsv', sep='\\t',encoding='utf-8', header=None)\n",
        "liar_test = pd.read_csv('/content/RUNI-NLP-FINAL-PROJECT/test.tsv', sep='\\t',encoding='utf-8', header=None)\n",
        "liar_valid = pd.read_csv('/content/RUNI-NLP-FINAL-PROJECT/valid.tsv', sep='\\t',encoding='utf-8', header=None)\n",
        "\n",
        "# Rename the columns to match the 'df_real' dataset\n",
        "liar_train.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
        "liar_valid.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context']\n",
        "liar_test.columns = ['id', 'label', 'statement', 'subject', 'speaker', 'job_title', 'state_info', 'party_affiliation', 'barely_true_counts', 'false_counts', 'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context']"
      ],
      "metadata": {
        "id": "kahiL2ea8-Ml"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Filter missing values (statistics below)"
      ],
      "metadata": {
        "id": "GnLXkCWL9KBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temp_real = len(df_real)\n",
        "temp_fake = len(df_fake)\n",
        "temp_li_train = len(liar_train)\n",
        "temp_li_val = len(liar_valid)\n",
        "temp_li_test = len(liar_test)\n",
        "\n",
        "df_real = df_real.dropna()\n",
        "df_fake = df_fake.dropna()\n",
        "liar_train = liar_train.dropna()\n",
        "liar_valid = liar_valid.dropna()\n",
        "liar_test = liar_test.dropna()\n",
        "\n",
        "g1 = temp_real - len(df_real)\n",
        "g2 = temp_fake - len(df_fake)\n",
        "g3 = temp_li_train - len(liar_train)\n",
        "g4 = temp_li_val - len(liar_valid)\n",
        "g5 = temp_li_test - len(liar_test)\n",
        "\n",
        "print(f'Number of df_real sentences: {len(df_real)}, removed {g1}({(g1/temp_real)*100:.1f}%) instances')\n",
        "print(f'Number of df_fake sentences: {len(df_fake)}, removed {g2}({(g1/temp_fake)*100:.1f}%) instances')\n",
        "print(f'Number of liar_train sentences: {len(liar_train)}, removed {g3}({(g3/temp_li_train)*100:.1f}%) instances')\n",
        "print(f'Number of liar_valid sentences: {len(liar_valid)}, removed {g4}({(g4/temp_li_val)*100:.1f}%) instances')\n",
        "print(f'Number of liar_test sentences: {len(liar_test)} removed {g5}({(g5/temp_li_test)*100:.1f}%) instances')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2DB2tVa9W4c",
        "outputId": "d7d2ca86-1783-4fb2-9949-07600e5a3ba4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of df_real sentences: 21417, removed 0(0.0%) instances\n",
            "Number of df_fake sentences: 23481, removed 0(0.0%) instances\n",
            "Number of liar_train sentences: 6724, removed 3516(34.3%) instances\n",
            "Number of liar_valid sentences: 861, removed 423(32.9%) instances\n",
            "Number of liar_test sentences: 853 removed 414(32.7%) instances\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datasets combinations"
      ],
      "metadata": {
        "id": "keMewjkF-GeP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UjL6A4sL-JJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_R365MCE93bp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}